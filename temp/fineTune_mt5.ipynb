{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aparna/miniconda3/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "# fine tune mt5 on dataset\n",
    "from transformers import MT5ForConditionalGeneration, MT5Tokenizer\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from simpletransformers.t5 import T5Model, T5Args\n",
    "from transformers import pipeline\n",
    "#import train split\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.preprocessing\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from google.transliteration import transliterate_word\n",
    "import klib\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>English_Translation</th>\n",
       "      <th>prefix</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@someUSER congratulations on you celebrating b...</td>\n",
       "      <td>@some users congratulate you for celebrating B...</td>\n",
       "      <td>translate English to Hinglish:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@LoKarDi_RT uske liye toh bahot kuch karna pad...</td>\n",
       "      <td>@Lokardi_ rat we should a lot more for that, b...</td>\n",
       "      <td>translate English to Hinglish:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@slimswamy yehi to hum semjhane ki koshish kar...</td>\n",
       "      <td>@Slimswami ehi, this is what i'm expecting you...</td>\n",
       "      <td>translate English to Hinglish:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@DramebaazKudi cake kaha hai ??</td>\n",
       "      <td>@Where is Dramebajakudi where is the cake?</td>\n",
       "      <td>translate English to Hinglish:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@someUSER i'm in hawaii at the moment .  home ...</td>\n",
       "      <td>@some user Don't want to come home next friday...</td>\n",
       "      <td>translate English to Hinglish:</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence  \\\n",
       "0  @someUSER congratulations on you celebrating b...   \n",
       "1  @LoKarDi_RT uske liye toh bahot kuch karna pad...   \n",
       "2  @slimswamy yehi to hum semjhane ki koshish kar...   \n",
       "3                    @DramebaazKudi cake kaha hai ??   \n",
       "4  @someUSER i'm in hawaii at the moment .  home ...   \n",
       "\n",
       "                                 English_Translation  \\\n",
       "0  @some users congratulate you for celebrating B...   \n",
       "1  @Lokardi_ rat we should a lot more for that, b...   \n",
       "2  @Slimswami ehi, this is what i'm expecting you...   \n",
       "3         @Where is Dramebajakudi where is the cake?   \n",
       "4  @some user Don't want to come home next friday...   \n",
       "\n",
       "                            prefix  \n",
       "0  translate English to Hinglish:   \n",
       "1  translate English to Hinglish:   \n",
       "2  translate English to Hinglish:   \n",
       "3  translate English to Hinglish:   \n",
       "4  translate English to Hinglish:   "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load dataset\n",
    "#English-Hindi code-mixed parallel corpus.csv\n",
    "df = pd.read_csv('PHNC/English-Hindi code-mixed parallel corpus.csv')\n",
    "df = df.dropna()\n",
    "df = df.reset_index(drop=True)\n",
    "# add column for prefix\n",
    "df['prefix'] = 'translate English to Hinglish: '\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of cleaned data: (13737, 2) - Remaining NAs: 0\n",
      "\n",
      "\n",
      "Dropped rows: 1\n",
      "     of which 1 duplicates. (Rows (first 150 shown): [1091])\n",
      "\n",
      "Dropped columns: 1\n",
      "     of which 1 single valued.     Columns: ['prefix']\n",
      "Dropped missing values: 0\n",
      "Reduced memory by at least: 0.1 MB (-32.26%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#data cleaning \n",
    "\n",
    "df=klib.data_cleaning(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8791\n",
      "2198\n",
      "2748\n"
     ]
    }
   ],
   "source": [
    "#split train, val, test\n",
    "# convert df  so that it can be used by transformers\n",
    "\n",
    "\n",
    "train, test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "train, val = train_test_split(train, test_size=0.2, random_state=42)\n",
    "train = train.reset_index(drop=True)\n",
    "val = val.reset_index(drop=True)\n",
    "test = test.reset_index(drop=True)\n",
    "\n",
    "#print lens\n",
    "print(len(train))\n",
    "print(len(val))\n",
    "print(len(test))\n",
    "\n",
    "#save train, val, test\n",
    "train.to_csv('train.csv', index=False)\n",
    "val.to_csv('val.csv', index=False)\n",
    "test.to_csv('test.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['sentence', 'english_translation'], dtype='object')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tokenize\n",
    "tokenizer = MT5Tokenizer.from_pretrained(\"google/mt5-small\")\n",
    "tokenizer.add_special_tokens({'additional_special_tokens': ['<sep>']})\n",
    "tokenizer.add_special_tokens({'additional_special_tokens': ['<pad>']})\n",
    "tokenizer.add_special_tokens({'additional_special_tokens': ['<s>']})\n",
    "tokenizer.add_special_tokens({'additional_special_tokens': ['</s>']})\n",
    "tokenizer.add_special_tokens({'additional_special_tokens': ['<unk>']})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        @someUSER congratulations on you celebrating b...\n",
       "1        @LoKarDi_RT uske liye toh bahot kuch karna pad...\n",
       "2        @slimswamy yehi to hum semjhane ki koshish kar...\n",
       "3                          @DramebaazKudi cake kaha hai ??\n",
       "4        @someUSER i'm in hawaii at the moment .  home ...\n",
       "                               ...                        \n",
       "13732    Dr Kumar Vishwas: \"Koi deewana kehta hai.. koi...\n",
       "13733    Me: Aaj kuch toofani karte hai.\n",
       "\n",
       "Mom: Pani ki ...\n",
       "13734    Pyar mangi to Jaan dengi,milk mango to kher de...\n",
       "13735     @imcomplicated__ kaale kaale baal gaal gore gore\n",
       "13736                            Ye sab aunty'on ke saath?\n",
       "Name: sentence, Length: 13737, dtype: string"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sentence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 512\n",
    "def tokenize_df(df):\n",
    "    target = tokenizer(df['sentence'], padding='max_length', truncation=True, return_tensors=\"pt\", max_length=maxlen)\n",
    "    input = tokenizer(df['english_translation'], padding='max_length', truncation=True, return_tensors=\"pt\", max_length=maxlen)\n",
    "    input_ids = input['input_ids']\n",
    "    attention_mask = input['attention_mask']\n",
    "    target_ids = target['input_ids']\n",
    "    target_attention_mask = target['attention_mask']\n",
    "    decoder_input_ids = target_ids.clone()\n",
    "    #convert to tensors\n",
    "    input_ids = torch.tensor(input_ids).squeeze()\n",
    "    attention_mask = torch.tensor(attention_mask).squeeze()\n",
    "    target_ids = torch.tensor(target_ids).squeeze()\n",
    "    target_attention_mask = torch.tensor(target_attention_mask).squeeze()\n",
    "   # decoder_input_ids = torch.tensor(decoder_input_ids)\n",
    "    \n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "        'labels': target_ids,\n",
    "        #'decoder_input_ids': decoder_input_ids,\n",
    "        #'decoder_attention_mask': target_attention_mask\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (/home/aparna/.cache/huggingface/datasets/csv/default-a95dd74f5c6e7a88/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n",
      "100%|██████████| 1/1 [00:00<00:00, 680.78it/s]\n",
      "Found cached dataset csv (/home/aparna/.cache/huggingface/datasets/csv/default-0292b3f7acb658fe/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n",
      "100%|██████████| 1/1 [00:00<00:00, 450.81it/s]\n",
      "Found cached dataset csv (/home/aparna/.cache/huggingface/datasets/csv/default-024ca5486b0891b6/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n",
      "100%|██████████| 1/1 [00:00<00:00, 574.01it/s]\n",
      "Map:   0%|          | 0/8791 [00:00<?, ? examples/s]/tmp/ipykernel_6105/3378448442.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_ids = torch.tensor(input_ids).squeeze()\n",
      "/tmp/ipykernel_6105/3378448442.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attention_mask = torch.tensor(attention_mask).squeeze()\n",
      "/tmp/ipykernel_6105/3378448442.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_ids = torch.tensor(target_ids).squeeze()\n",
      "/tmp/ipykernel_6105/3378448442.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_attention_mask = torch.tensor(target_attention_mask).squeeze()\n",
      "                                                                 \r"
     ]
    }
   ],
   "source": [
    "#tokenize train, val, test\n",
    "train = load_dataset('csv', data_files='train.csv')\n",
    "val = load_dataset('csv', data_files='val.csv')\n",
    "test = load_dataset('csv', data_files='test.csv')\n",
    "train = train.map(tokenize_df, batched=True, batch_size=128,remove_columns=['sentence','english_translation'])\n",
    "val = val.map(tokenize_df, batched=True, batch_size=128,remove_columns=['sentence','english_translation'])\n",
    "test = test.map(tokenize_df, batched=True, batch_size=128,remove_columns=['sentence','english_translation'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n",
      "512\n"
     ]
    }
   ],
   "source": [
    "train\n",
    "#get sample \n",
    "sample = train['train'][0]\n",
    "sample\n",
    "#print shapes\n",
    "print(len(sample['input_ids']))\n",
    "print(len(sample['attention_mask']))\n",
    "#print(len(sample['decoder_input_ids']))\n",
    "#print(len(sample['decoder_attention_mask']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 2198\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "# batch_size = 8\n",
    "# train_dataloader = DataLoader(\n",
    "\n",
    "#             train,  # The training samples.\n",
    "\n",
    "#             sampler = RandomSampler(train), # Select batches randomly\n",
    "\n",
    "#             batch_size = batch_size # Trains with this batch size.\n",
    "\n",
    "#         )\n",
    "\n",
    "# validation_dataloader = DataLoader(\n",
    "\n",
    "#             val, # The validation samples.\n",
    "\n",
    "#             sampler = SequentialSampler(val), # Pull out batches sequentially.\n",
    "\n",
    "#             batch_size = batch_size # Evaluate with this batch size.\n",
    "\n",
    "#         )\n",
    "\n",
    "# test_dataloader = DataLoader(\n",
    "\n",
    "\n",
    "#             test, # The validation samples. \n",
    "\n",
    "#             sampler = SequentialSampler(test), # Pull out batches sequentially.\n",
    "\n",
    "#             batch_size = batch_size # Evaluate with this batch size.\n",
    "\n",
    "#         )\n",
    "\n",
    "# #test train data loader\n",
    "# for batch in train_dataloader:\n",
    "\n",
    "#     print(batch)\n",
    "\n",
    "#     break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /home/aparna/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--rouge/b01e0accf3bd6dd24839b769a5fda24e14995071570870922c71970b3a6ed886 (last modified on Mon Mar 20 18:02:43 2023) since it couldn't be found locally at evaluate-metric--rouge, or remotely on the Hugging Face Hub.\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "\n",
    "def tokenize_sentence(arg):\n",
    "    encoded_arg =tokenizer(arg)\n",
    "    return tokenizer.convert_ids_to_tokens(encoded_arg.input_ids)\n",
    "\n",
    "def metrics_func(eval_arg):\n",
    "    preds, labels = eval_arg\n",
    "    # Replace -100\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    # Convert id tokens to text\n",
    "    text_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    text_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    # Insert a line break (\\n) in each sentence for ROUGE scoring\n",
    "    # (Note : Please change this code, when you perform on other languages except for Japanese)\n",
    "    text_preds = [(p if p.endswith((\"!\", \"！\", \"?\", \"？\", \"。\")) else p + \"。\") for p in text_preds]\n",
    "    text_labels = [(l if l.endswith((\"!\", \"！\", \"?\", \"？\", \"。\")) else l + \"。\") for l in text_labels]\n",
    "    sent_tokenizer_jp = RegexpTokenizer(u'[^!！?？。]*[!！?？。]')\n",
    "    text_preds = [\"\\n\".join(np.char.strip(sent_tokenizer_jp.tokenize(p))) for p in text_preds]\n",
    "    text_labels = [\"\\n\".join(np.char.strip(sent_tokenizer_jp.tokenize(l))) for l in text_labels]\n",
    "    # compute ROUGE score with custom tokenization\n",
    "    return rouge_metric.compute(\n",
    "        predictions=text_preds,\n",
    "        references=text_labels,\n",
    "        tokenizer=tokenize_sentence\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/2740 [12:55<196:31:16, 258.49s/it]\n"
     ]
    }
   ],
   "source": [
    "# finetuen mt5\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "model = MT5ForConditionalGeneration.from_pretrained(\"google/mt5-small\")\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "#training args\n",
    "\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "  output_dir = \"mt5-codemixed\",\n",
    "  log_level = \"error\",\n",
    "  num_train_epochs = 10,\n",
    "  learning_rate = 5e-4,\n",
    "  lr_scheduler_type = \"linear\",\n",
    "  warmup_steps = 90,\n",
    "  optim = \"adafactor\",\n",
    "  weight_decay = 0.01,\n",
    "  per_device_train_batch_size = 2,\n",
    "  per_device_eval_batch_size = 1,\n",
    "  gradient_accumulation_steps = 16,\n",
    "  evaluation_strategy = \"steps\",\n",
    "  eval_steps = 100,\n",
    "  predict_with_generate=True,\n",
    "  generation_max_length = 128,\n",
    "  save_steps = 500,\n",
    "  logging_steps = 10,\n",
    "  push_to_hub = False\n",
    ")\n",
    "\n",
    "\n",
    "#trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train[\"train\"],         # training dataset\n",
    "    eval_dataset=val[\"train\"],             # evaluation dataset\n",
    "    tokenizer=tokenizer,               # tokenizer\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer, model=model), # data collator\n",
    "    \n",
    ")\n",
    "\n",
    "#train\n",
    "trainer.train()\n",
    "\n",
    "#save model\n",
    "trainer.save_model(\"./mt5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[     0,   1250,   7800, 198280,    259,  65334,    260,   1061,  39084,\n",
      "          19349,    318,  51571,  62342,  16263, 144044,    603,   2148,    513,\n",
      "           2941,    334,   1312,  88806,   2050,    432,    262,    268,    387,\n",
      "           1759,    290, 165794,      1,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0],\n",
      "        [     0,  27696,  62342,    259,    261,    342,    776, 113865,    714,\n",
      "           2829,    387, 102339,   9065,  42716,    260,   1816,    321,  12961,\n",
      "            623,   3663,    479,   1776,   1250,   6253, 182594,    262,    290,\n",
      "           1795,   1061, 146525,  56696,    313,  11395,    330,  35714,    609,\n",
      "           1350,    311,      1],\n",
      "        [     0,   1250,  38393,    265,    299,    609,    339,   4592,   6504,\n",
      "            259,   1542,    787,   3007,    288,   6313,    260,   1061,    559,\n",
      "            604,    263, 152418,   7925,   7954,      1,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0],\n",
      "        [     0,    387,  96125,   4748,  30875,   1711,    714,  16097,    339,\n",
      "            609,   6644,    288,    783,  58500,   5155,    281,   5169,  11495,\n",
      "              1,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0],\n",
      "        [     0,   1250,  16403, 210381,    584,  26112,  10060,   2050,    259,\n",
      "          97308,   1776,      1,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0]]) tensor([[  1250,   7800, 198280,  ...,      0,      0,      0],\n",
      "        [ 11674,   6101,   2050,  ...,      0,      0,      0],\n",
      "        [  1250,  38393,    265,  ...,      0,      0,      0],\n",
      "        [   387,  96125,   4748,  ...,      0,      0,      0],\n",
      "        [  1250,  16403, 210381,  ...,      0,      0,      0]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'rouge1': 0.47184469277492536,\n",
       " 'rouge2': 0.32362358952522885,\n",
       " 'rougeL': 0.44858887882143694,\n",
       " 'rougeLsum': 0.44858887882143694}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "model = MT5ForConditionalGeneration.from_pretrained(\"./mt5v1\")\n",
    "#tokenizer = MT5Tokenizer.from_pretrained(\"./mt5\")\n",
    "\n",
    "\n",
    "sample_dataloader = DataLoader(\n",
    "  test[\"train\"].with_format(\"torch\"),\n",
    "  collate_fn=DataCollatorForSeq2Seq(tokenizer, model=model),\n",
    "  batch_size=5)\n",
    "for batch in sample_dataloader:\n",
    "  with torch.no_grad():\n",
    "    preds = model.generate(\n",
    "      batch[\"input_ids\"],\n",
    "      num_beams=15,\n",
    "      num_return_sequences=1,\n",
    "      no_repeat_ngram_size=1,\n",
    "      remove_invalid_values=True,\n",
    "      max_length=128,\n",
    "    )\n",
    "  labels = batch[\"labels\"]\n",
    "  break\n",
    "print(preds, labels)\n",
    "metrics_func([preds, labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Input's Text *****\n",
      "@rynkee it is thi thought which we want to change. @PunsTurnMeOn\n",
      "***** codemix (True Value) *****\n",
      "@rynkee yehi soch to badalni hai @PunsTurnMeOn\n",
      "***** codemix (Generated Text) *****\n",
      "@rynkees it is thi thought which we want to change.@PunsTurnMeOn\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Predict with test data (first 5 rows)\n",
    "sample_dataloader = DataLoader(\n",
    "  test[\"train\"].with_format(\"torch\"),\n",
    "  collate_fn=DataCollatorForSeq2Seq(tokenizer, model=model),\n",
    "  batch_size=5)\n",
    "for batch in sample_dataloader:\n",
    "  with torch.no_grad():\n",
    "    preds = model.generate(\n",
    "      batch[\"input_ids\"],\n",
    "      num_beams=15,\n",
    "      num_return_sequences=1,\n",
    "      no_repeat_ngram_size=1,\n",
    "      remove_invalid_values=True,\n",
    "      max_length=128,\n",
    "    )\n",
    "  labels = batch[\"labels\"]\n",
    "  inputs = batch[\"input_ids\"]\n",
    "  break\n",
    "\n",
    "# Replace -100 (see above)\n",
    "inputs = np.where(inputs != -100, inputs, tokenizer.pad_token_id)\n",
    "labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "\n",
    "# Convert id tokens to text\n",
    "text_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "text_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "text_inputs = tokenizer.batch_decode(inputs, skip_special_tokens=True)\n",
    "\n",
    "# Show result\n",
    "print(\"***** Input's Text *****\")\n",
    "print(text_inputs[2])\n",
    "print(\"***** codemix (True Value) *****\")\n",
    "print(text_labels[2])\n",
    "print(\"***** codemix (Generated Text) *****\")\n",
    "print(text_preds[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Input's Text *****\n",
      "@hurdangi haan.. @sagarikaghose sister will eat green mango today @the_hindu\n",
      "***** codemix (True Value) *****\n",
      "@hurdangi haan.. @sagarikaghose Didi aaj hare rang ke aam khaengi @the_hindu\n",
      "***** codemix (Generated Text) *****\n",
      "@hurdangi haan.@sagarikaghose bhai green mango peene ke saath kharab kar jaao #the_hindu\n",
      "***** Input's Text *****\n",
      "wait brother, do not cry this much, its #GST not a bomb. have some shame. @digvijaya_28 @INCIndia \" country brought it out \"now you sit and cry\n",
      "***** codemix (True Value) *****\n",
      "Are bas kar bhai itna nahi rone \"ka #GST hai bomb nahi. Kuch to sharm karo. @digvijaya_28 @INCIndia \" desh nikal liya \"aage u sit and cry\n",
      "***** codemix (Generated Text) *****\n",
      "wait bhai, do not cry this much #GST nahi bomb. Haan kuch ho chuka hai @digvijaya_28@INCIndia \" country brought it out\"\n",
      "***** Input's Text *****\n",
      "@rynkee it is thi thought which we want to change. @PunsTurnMeOn\n",
      "***** codemix (True Value) *****\n",
      "@rynkee yehi soch to badalni hai @PunsTurnMeOn\n",
      "***** codemix (Generated Text) *****\n",
      "@rynkees it is thi thought which we want to change.@PunsTurnMeOn\n",
      "***** Input's Text *****\n",
      "#RightToPrivacy under this act, is it possible to have urinals in open space\n",
      "***** codemix (True Value) *****\n",
      "#RightToPrivacy ke andar ab khule me susu allow hai kya?\n",
      "***** codemix (Generated Text) *****\n",
      "#RightToPrivacy under this act is it possible to have urinals in open space\n",
      "***** Input's Text *****\n",
      "@FreeCharge recharge too.. fun too#MyCaptionForFreeCharge\n",
      "***** codemix (True Value) *****\n",
      "@FreeCharge recharge bhi.. Maze bhi #MyCaptionForFreeCharge\n",
      "***** codemix (Generated Text) *****\n",
      "@FreeCharge recharge bhi kar rahe hai\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(\"***** Input's Text *****\")\n",
    "    print(text_inputs[i])\n",
    "    print(\"***** codemix (True Value) *****\")\n",
    "    print(text_labels[i])\n",
    "    print(\"***** codemix (Generated Text) *****\")\n",
    "    print(text_preds[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
