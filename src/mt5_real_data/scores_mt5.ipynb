{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aparna/miniconda3/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "# fine tune mt5 on dataset\n",
    "from transformers import MT5ForConditionalGeneration, MT5Tokenizer\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from simpletransformers.t5 import T5Model, T5Args\n",
    "from transformers import pipeline\n",
    "#import train split\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.preprocessing\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from google.transliteration import transliterate_word\n",
    "import klib\n",
    "import os\n",
    "#bleu score\n",
    "from torchtext.data.metrics import bleu_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize\n",
    "tokenizer = MT5Tokenizer.from_pretrained(\"google/mt5-small\")\n",
    "tokenizer.add_special_tokens({'additional_special_tokens': ['<sep>']})\n",
    "tokenizer.add_special_tokens({'additional_special_tokens': ['<pad>']})\n",
    "tokenizer.add_special_tokens({'additional_special_tokens': ['<s>']})\n",
    "tokenizer.add_special_tokens({'additional_special_tokens': ['</s>']})\n",
    "tokenizer.add_special_tokens({'additional_special_tokens': ['<unk>']})\n",
    "\n",
    "maxlen = 512\n",
    "def tokenize_df(df):\n",
    "    target = tokenizer(df['sentence'], padding='max_length', truncation=True, return_tensors=\"pt\", max_length=maxlen)\n",
    "    input = tokenizer(df['english_translation'], padding='max_length', truncation=True, return_tensors=\"pt\", max_length=maxlen)\n",
    "    input_ids = input['input_ids']\n",
    "    attention_mask = input['attention_mask']\n",
    "    target_ids = target['input_ids']\n",
    "    target_attention_mask = target['attention_mask']\n",
    "    decoder_input_ids = target_ids.clone()\n",
    "    #convert to tensors\n",
    "    input_ids = torch.tensor(input_ids).squeeze()\n",
    "    attention_mask = torch.tensor(attention_mask).squeeze()\n",
    "    target_ids = torch.tensor(target_ids).squeeze()\n",
    "    target_attention_mask = torch.tensor(target_attention_mask).squeeze()\n",
    "   # decoder_input_ids = torch.tensor(decoder_input_ids)\n",
    "    \n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "        'labels': target_ids,\n",
    "        #'decoder_input_ids': decoder_input_ids,\n",
    "        #'decoder_attention_mask': target_attention_mask\n",
    "    }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (/home/aparna/.cache/huggingface/datasets/csv/default-27c3049462760e81/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n",
      "100%|██████████| 1/1 [00:00<00:00, 242.66it/s]\n",
      "Found cached dataset csv (/home/aparna/.cache/huggingface/datasets/csv/default-e471cfb17a39f3d5/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n",
      "100%|██████████| 1/1 [00:00<00:00, 299.46it/s]\n",
      "Found cached dataset csv (/home/aparna/.cache/huggingface/datasets/csv/default-7fff95b9c64c70e2/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n",
      "100%|██████████| 1/1 [00:00<00:00, 399.42it/s]\n",
      "Loading cached processed dataset at /home/aparna/.cache/huggingface/datasets/csv/default-27c3049462760e81/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-28492876665d7691.arrow\n",
      "Loading cached processed dataset at /home/aparna/.cache/huggingface/datasets/csv/default-e471cfb17a39f3d5/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-e28e6c52145254ef.arrow\n",
      "Loading cached processed dataset at /home/aparna/.cache/huggingface/datasets/csv/default-7fff95b9c64c70e2/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-063947ebd8255499.arrow\n"
     ]
    }
   ],
   "source": [
    "train = load_dataset('csv', data_files='train.csv')\n",
    "val = load_dataset('csv', data_files='val.csv')\n",
    "test = load_dataset('csv', data_files='test.csv')\n",
    "train = train.map(tokenize_df, batched=True, batch_size=128,remove_columns=['sentence','english_translation'])\n",
    "val = val.map(tokenize_df, batched=True, batch_size=128,remove_columns=['sentence','english_translation'])\n",
    "test = test.map(tokenize_df, batched=True, batch_size=128,remove_columns=['sentence','english_translation'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "\n",
    "def tokenize_sentence(arg):\n",
    "    encoded_arg =tokenizer(arg)\n",
    "    return tokenizer.convert_ids_to_tokens(encoded_arg.input_ids)\n",
    "\n",
    "def metrics_func(eval_arg):\n",
    "    preds, labels = eval_arg\n",
    "    # Replace -100\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    # Convert id tokens to text\n",
    "    text_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    text_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    # Insert a line break (\\n) in each sentence for ROUGE scoring\n",
    "    # (Note : Please change this code, when you perform on other languages except for Japanese)\n",
    "    text_preds = [(p if p.endswith((\"!\", \"！\", \"?\", \"？\", \"。\")) else p + \"。\") for p in text_preds]\n",
    "    text_labels = [(l if l.endswith((\"!\", \"！\", \"?\", \"？\", \"。\")) else l + \"。\") for l in text_labels]\n",
    "    sent_tokenizer_jp = RegexpTokenizer(u'[^!！?？。]*[!！?？。]')\n",
    "    text_preds = [\"\\n\".join(np.char.strip(sent_tokenizer_jp.tokenize(p))) for p in text_preds]\n",
    "    text_labels = [\"\\n\".join(np.char.strip(sent_tokenizer_jp.tokenize(l))) for l in text_labels]\n",
    "    # compute ROUGE score with custom tokenization\n",
    "    #blue score\n",
    "    #TOKENS FOR BLEU\n",
    "    text_preds_TOKENS = [tokenize_sentence(p) for p in text_preds]\n",
    "    text_labels_TOKENS = [tokenize_sentence(l) for l in text_labels]\n",
    "    \n",
    "    return rouge_metric.compute(\n",
    "        predictions=text_preds,\n",
    "        references=text_labels,\n",
    "        tokenizer=tokenize_sentence\n",
    "    ),bleu_score(text_labels_TOKENS,text_preds_TOKENS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "#tokenizer = MT5Tokenizer.from_pretrained(\"./mt5\")\n",
    "def testing(model):\n",
    "    metrics =[]\n",
    "    sample_dataloader = DataLoader(\n",
    "      test[\"train\"].with_format(\"torch\"),\n",
    "      collate_fn=DataCollatorForSeq2Seq(tokenizer, model=model),\n",
    "      batch_size=5)\n",
    "    for batch in sample_dataloader:\n",
    "      with torch.no_grad():\n",
    "        preds = model.generate(\n",
    "          batch[\"input_ids\"],\n",
    "          num_beams=15,\n",
    "          num_return_sequences=1,\n",
    "          no_repeat_ngram_size=1,\n",
    "          remove_invalid_values=True,\n",
    "          max_length=128,\n",
    "        )\n",
    "      labels = batch[\"labels\"]\n",
    "      metric = metrics_func([preds, labels])\n",
    "      metrics.append(metric)\n",
    "    return metrics\n",
    "\n",
    "def average_metric(metrics):\n",
    "    rouge = 0\n",
    "    rouge2 = 0\n",
    "    rougeL = 0\n",
    "    rougeLsum = 0\n",
    "    bleu = 0\n",
    "    for metric in metrics:\n",
    "        rouge += metric[0]['rouge1']\n",
    "        rouge2 += metric[0]['rouge2']\n",
    "        rougeL += metric[0]['rougeL']\n",
    "        rougeLsum += metric[0]['rougeLsum']\n",
    "        bleu += metric[1]\n",
    "    return rouge/len(metrics),rouge2/len(metrics),rougeL/len(metrics),rougeLsum/len(metrics),bleu/len(metrics)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "invalid index to scalar variable.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[39m=\u001b[39m MT5ForConditionalGeneration\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39m./mt5v1\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m metrics \u001b[39m=\u001b[39m testing(model)\n\u001b[0;32m----> 3\u001b[0m scores \u001b[39m=\u001b[39m average_metric(metrics)\n",
      "Cell \u001b[0;32mIn[7], line 31\u001b[0m, in \u001b[0;36maverage_metric\u001b[0;34m(metrics)\u001b[0m\n\u001b[1;32m     29\u001b[0m bleu \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     30\u001b[0m \u001b[39mfor\u001b[39;00m metric \u001b[39min\u001b[39;00m metrics:\n\u001b[0;32m---> 31\u001b[0m     rouge \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m metric[\u001b[39m0\u001b[39;49m][\u001b[39m'\u001b[39;49m\u001b[39mrouge1\u001b[39;49m\u001b[39m'\u001b[39;49m][\u001b[39m2\u001b[39;49m]\n\u001b[1;32m     32\u001b[0m     rouge2 \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m metric[\u001b[39m0\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mrouge2\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m2\u001b[39m]\n\u001b[1;32m     33\u001b[0m     rougeL \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m metric[\u001b[39m0\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mrougeL\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m2\u001b[39m]\n",
      "\u001b[0;31mIndexError\u001b[0m: invalid index to scalar variable."
     ]
    }
   ],
   "source": [
    "model = MT5ForConditionalGeneration.from_pretrained(\"./mt5v1\")\n",
    "metrics = testing(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "basecase_mt5\n",
      "rouge: 0.4729317578176422\n",
      "rouge2: 0.2991320730642171\n",
      "rougeL: 0.4538605700654378\n",
      "rougeLsum: 0.4561573986969596\n",
      "bleu: 0.0\n"
     ]
    }
   ],
   "source": [
    "print(\"basecase_mt5\")\n",
    "scores = average_metric(metrics)\n",
    "print(\"rouge:\",scores[0])\n",
    "print(\"rouge2:\",scores[1])\n",
    "print(\"rougeL:\",scores[2])\n",
    "print(\"rougeLsum:\",scores[3])\n",
    "print(\"bleu:\",scores[4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 30\u001b[0m\n\u001b[1;32m     28\u001b[0m text_labels \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mbatch_decode(labels, skip_special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     29\u001b[0m text_inputs \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mbatch_decode(inputs, skip_special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> 30\u001b[0m \u001b[39mprint\u001b[39m(sentence_bleu(\u001b[39mlist\u001b[39m(text_labels\u001b[39m.\u001b[39;49msplit()),\u001b[39mlist\u001b[39m(text_preds\u001b[39m.\u001b[39msplit())))\n\u001b[1;32m     32\u001b[0m \u001b[39m# Show result\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m***** Input\u001b[39m\u001b[39m'\u001b[39m\u001b[39ms Text *****\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Predict with test data (first 5 rows)\n",
    "sample_dataloader = DataLoader(\n",
    "  test[\"train\"].with_format(\"torch\"),\n",
    "  collate_fn=DataCollatorForSeq2Seq(tokenizer, model=model),\n",
    "  batch_size=5)\n",
    "for batch in sample_dataloader:\n",
    "  with torch.no_grad():\n",
    "    preds = model.generate(\n",
    "      batch[\"input_ids\"],\n",
    "      num_beams=15,\n",
    "      num_return_sequences=1,\n",
    "      no_repeat_ngram_size=1,\n",
    "      remove_invalid_values=True,\n",
    "      max_length=128,\n",
    "    )\n",
    "  labels = batch[\"labels\"]\n",
    "  inputs = batch[\"input_ids\"]\n",
    "  break\n",
    "\n",
    "# Replace -100 (see above)\n",
    "inputs = np.where(inputs != -100, inputs, tokenizer.pad_token_id)\n",
    "labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "\n",
    "# Convert id tokens to text\n",
    "text_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "text_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "text_inputs = tokenizer.batch_decode(inputs, skip_special_tokens=True)\n",
    "print(bleu_score(list(text_labels.split()),list(text_preds.split())))\n",
    "\n",
    "# Show result\n",
    "print(\"***** Input's Text *****\")\n",
    "print(text_inputs[2])\n",
    "print(\"***** codemix (True Value) *****\")\n",
    "print(text_labels[2])\n",
    "print(\"***** codemix (Generated Text) *****\")\n",
    "print(text_preds[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Input's Text *****\n",
      "@hurdangi haan.. @sagarikaghose sister will eat green mango today @the_hindu\n",
      "***** codemix (True Value) *****\n",
      "@hurdangi haan.. @sagarikaghose Didi aaj hare rang ke aam khaengi @the_hindu\n",
      "***** codemix (Generated Text) *****\n",
      "@hurdangi haan.@sagarikaghose bhai green mango peene ke saath kharab kar jaao #the_hindu\n",
      "***** Input's Text *****\n",
      "wait brother, do not cry this much, its #GST not a bomb. have some shame. @digvijaya_28 @INCIndia \" country brought it out \"now you sit and cry\n",
      "***** codemix (True Value) *****\n",
      "Are bas kar bhai itna nahi rone \"ka #GST hai bomb nahi. Kuch to sharm karo. @digvijaya_28 @INCIndia \" desh nikal liya \"aage u sit and cry\n",
      "***** codemix (Generated Text) *****\n",
      "wait bhai, do not cry this much #GST nahi bomb. Haan kuch ho chuka hai @digvijaya_28@INCIndia \" country brought it out\"\n",
      "***** Input's Text *****\n",
      "@rynkee it is thi thought which we want to change. @PunsTurnMeOn\n",
      "***** codemix (True Value) *****\n",
      "@rynkee yehi soch to badalni hai @PunsTurnMeOn\n",
      "***** codemix (Generated Text) *****\n",
      "@rynkees it is thi thought which we want to change.@PunsTurnMeOn\n",
      "***** Input's Text *****\n",
      "#RightToPrivacy under this act, is it possible to have urinals in open space\n",
      "***** codemix (True Value) *****\n",
      "#RightToPrivacy ke andar ab khule me susu allow hai kya?\n",
      "***** codemix (Generated Text) *****\n",
      "#RightToPrivacy under this act is it possible to have urinals in open space\n",
      "***** Input's Text *****\n",
      "@FreeCharge recharge too.. fun too#MyCaptionForFreeCharge\n",
      "***** codemix (True Value) *****\n",
      "@FreeCharge recharge bhi.. Maze bhi #MyCaptionForFreeCharge\n",
      "***** codemix (Generated Text) *****\n",
      "@FreeCharge recharge bhi kar rahe hai\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(\"***** Input's Text *****\")\n",
    "    print(text_inputs[i])\n",
    "    print(\"***** codemix (True Value) *****\")\n",
    "    print(text_labels[i])\n",
    "    print(\"***** codemix (Generated Text) *****\")\n",
    "    print(text_preds[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
