{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "# fine tune mt5 on dataset\n",
    "from transformers import MT5ForConditionalGeneration, MT5Tokenizer, BertTokenizer, BertModel\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import BertGenerationEncoder,BertGenerationDecoder,EncoderDecoderModel\n",
    "from datasets import load_dataset\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from simpletransformers.t5 import T5Model, T5Args\n",
    "from transformers import pipeline\n",
    "#import train split\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.preprocessing\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from google.transliteration import transliterate_word\n",
    "import klib\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>English_Translation</th>\n",
       "      <th>prefix</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@someUSER congratulations on you celebrating b...</td>\n",
       "      <td>@some users congratulate you for celebrating B...</td>\n",
       "      <td>translate English to Hinglish:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@LoKarDi_RT uske liye toh bahot kuch karna pad...</td>\n",
       "      <td>@Lokardi_ rat we should a lot more for that, b...</td>\n",
       "      <td>translate English to Hinglish:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@slimswamy yehi to hum semjhane ki koshish kar...</td>\n",
       "      <td>@Slimswami ehi, this is what i'm expecting you...</td>\n",
       "      <td>translate English to Hinglish:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@DramebaazKudi cake kaha hai ??</td>\n",
       "      <td>@Where is Dramebajakudi where is the cake?</td>\n",
       "      <td>translate English to Hinglish:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@someUSER i'm in hawaii at the moment .  home ...</td>\n",
       "      <td>@some user Don't want to come home next friday...</td>\n",
       "      <td>translate English to Hinglish:</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence  \\\n",
       "0  @someUSER congratulations on you celebrating b...   \n",
       "1  @LoKarDi_RT uske liye toh bahot kuch karna pad...   \n",
       "2  @slimswamy yehi to hum semjhane ki koshish kar...   \n",
       "3                    @DramebaazKudi cake kaha hai ??   \n",
       "4  @someUSER i'm in hawaii at the moment .  home ...   \n",
       "\n",
       "                                 English_Translation  \\\n",
       "0  @some users congratulate you for celebrating B...   \n",
       "1  @Lokardi_ rat we should a lot more for that, b...   \n",
       "2  @Slimswami ehi, this is what i'm expecting you...   \n",
       "3         @Where is Dramebajakudi where is the cake?   \n",
       "4  @some user Don't want to come home next friday...   \n",
       "\n",
       "                            prefix  \n",
       "0  translate English to Hinglish:   \n",
       "1  translate English to Hinglish:   \n",
       "2  translate English to Hinglish:   \n",
       "3  translate English to Hinglish:   \n",
       "4  translate English to Hinglish:   "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load dataset\n",
    "#English-Hindi code-mixed parallel corpus.csv\n",
    "df = pd.read_csv('../PHNC/English-Hindi code-mixed parallel corpus.csv')\n",
    "df = df.dropna()\n",
    "df = df.reset_index(drop=True)\n",
    "# add column for prefix\n",
    "df['prefix'] = 'translate English to Hinglish: '\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of cleaned data: (13737, 2) - Remaining NAs: 0\n",
      "\n",
      "\n",
      "Dropped rows: 1\n",
      "     of which 1 duplicates. (Rows (first 150 shown): [1091])\n",
      "\n",
      "Dropped columns: 1\n",
      "     of which 1 single valued.     Columns: ['prefix']\n",
      "Dropped missing values: 0\n",
      "Reduced memory by at least: 0.1 MB (-32.26%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#data cleaning \n",
    "\n",
    "df=klib.data_cleaning(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8791\n",
      "2198\n",
      "2748\n"
     ]
    }
   ],
   "source": [
    "#split train, val, test\n",
    "# convert df  so that it can be used by transformers\n",
    "\n",
    "\n",
    "train, test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "train, val = train_test_split(train, test_size=0.2, random_state=42)\n",
    "train = train.reset_index(drop=True)\n",
    "val = val.reset_index(drop=True)\n",
    "test = test.reset_index(drop=True)\n",
    "\n",
    "#print lens\n",
    "print(len(train))\n",
    "print(len(val))\n",
    "print(len(test))\n",
    "\n",
    "#save train, val, test\n",
    "train.to_csv('../train.csv', index=False)\n",
    "val.to_csv('../val.csv', index=False)\n",
    "test.to_csv('..//test.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['sentence', 'english_translation'], dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tokenize\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "tokenizer.add_special_tokens({'additional_special_tokens': ['<sep>']})\n",
    "tokenizer.add_special_tokens({'additional_special_tokens': ['<pad>']})\n",
    "tokenizer.add_special_tokens({'additional_special_tokens': ['<s>']})\n",
    "tokenizer.add_special_tokens({'additional_special_tokens': ['</s>']})\n",
    "tokenizer.add_special_tokens({'additional_special_tokens': ['<unk>']})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        @someUSER congratulations on you celebrating b...\n",
       "1        @LoKarDi_RT uske liye toh bahot kuch karna pad...\n",
       "2        @slimswamy yehi to hum semjhane ki koshish kar...\n",
       "3                          @DramebaazKudi cake kaha hai ??\n",
       "4        @someUSER i'm in hawaii at the moment .  home ...\n",
       "                               ...                        \n",
       "13732    Dr Kumar Vishwas: \"Koi deewana kehta hai.. koi...\n",
       "13733    Me: Aaj kuch toofani karte hai.\n",
       "\n",
       "Mom: Pani ki ...\n",
       "13734    Pyar mangi to Jaan dengi,milk mango to kher de...\n",
       "13735     @imcomplicated__ kaale kaale baal gaal gore gore\n",
       "13736                            Ye sab aunty'on ke saath?\n",
       "Name: sentence, Length: 13737, dtype: string"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sentence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Original:  @someUSER congratulations on you celebrating british kid singers sophia grace's and rosie's 1st anniversary of a visit of your show .  how\n",
      "Tokenized:  ['@', 'some', '##US', '##ER', 'con', '##grat', '##ulations', 'on', 'you', 'celebra', '##ting', 'brit', '##ish', 'ki', '##d', 'singers', 'so', '##phia', 'gra', '##ce', \"'\", 's', 'and', 'ros', '##ie', \"'\", 's', '1st', 'anniversary', 'of', 'a', 'visit', 'of', 'your', 'show', '.', 'how']\n",
      "Token IDs:  [137, 11152, 32612, 24093, 10173, 84558, 74874, 10135, 13028, 42892, 12141, 52676, 15529, 10879, 10162, 81156, 10380, 89224, 63706, 10419, 112, 187, 10111, 39312, 10400, 112, 187, 13510, 37157, 10108, 169, 27541, 10108, 20442, 11897, 119, 14796]\n"
     ]
    }
   ],
   "source": [
    "# Print the original sentence.\n",
    "print(' Original: ', df['sentence'][0])\n",
    "\n",
    "# Print the tweet split into tokens.\n",
    "print('Tokenized: ', tokenizer.tokenize(df['sentence'][0]))\n",
    "\n",
    "# Print the tweet mapped to token ids.\n",
    "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(df['sentence'][0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 512\n",
    "def tokenize_df(df):\n",
    "    target = tokenizer(df['sentence'], padding='max_length', truncation=True, return_tensors=\"pt\", max_length=maxlen)\n",
    "    input = tokenizer(df['english_translation'], padding='max_length', truncation=True, return_tensors=\"pt\", max_length=maxlen)\n",
    "    input_ids = input['input_ids']\n",
    "    attention_mask = input['attention_mask']\n",
    "    target_ids = target['input_ids']\n",
    "    target_attention_mask = target['attention_mask']\n",
    "    decoder_input_ids = target_ids.clone()\n",
    "    #convert to tensors\n",
    "    input_ids = torch.tensor(input_ids).squeeze()\n",
    "    attention_mask = torch.tensor(attention_mask).squeeze()\n",
    "    target_ids = torch.tensor(target_ids).squeeze()\n",
    "    target_attention_mask = torch.tensor(target_attention_mask).squeeze()\n",
    "   # decoder_input_ids = torch.tensor(decoder_input_ids)\n",
    "    \n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "        'labels': target_ids,\n",
    "        #'decoder_input_ids': decoder_input_ids,\n",
    "        #'decoder_attention_mask': target_attention_mask\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to /home/aparna/.cache/huggingface/datasets/csv/default-02482a7dc6cd63f0/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 5915.80it/s]\n",
      "Extracting data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 966.21it/s]\n",
      "Generating train split: 0 examples [00:00, ? examples/s]/home/aparna/miniconda3/lib/python3.10/site-packages/datasets/download/streaming_download_manager.py:776: FutureWarning: the 'mangle_dupe_cols' keyword is deprecated and will be removed in a future version. Please take steps to stop the use of 'mangle_dupe_cols'\n",
      "  return pd.read_csv(xopen(filepath_or_buffer, \"rb\", use_auth_token=use_auth_token), **kwargs)\n",
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /home/aparna/.cache/huggingface/datasets/csv/default-02482a7dc6cd63f0/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 400.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to /home/aparna/.cache/huggingface/datasets/csv/default-a6c55461f7455f05/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 3960.63it/s]\n",
      "Extracting data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 655.05it/s]\n",
      "Generating train split: 0 examples [00:00, ? examples/s]/home/aparna/miniconda3/lib/python3.10/site-packages/datasets/download/streaming_download_manager.py:776: FutureWarning: the 'mangle_dupe_cols' keyword is deprecated and will be removed in a future version. Please take steps to stop the use of 'mangle_dupe_cols'\n",
      "  return pd.read_csv(xopen(filepath_or_buffer, \"rb\", use_auth_token=use_auth_token), **kwargs)\n",
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /home/aparna/.cache/huggingface/datasets/csv/default-a6c55461f7455f05/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 394.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to /home/aparna/.cache/huggingface/datasets/csv/default-18d6eb5fdf4808e6/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 1337.90it/s]\n",
      "Extracting data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 753.15it/s]\n",
      "Generating train split: 0 examples [00:00, ? examples/s]/home/aparna/miniconda3/lib/python3.10/site-packages/datasets/download/streaming_download_manager.py:776: FutureWarning: the 'mangle_dupe_cols' keyword is deprecated and will be removed in a future version. Please take steps to stop the use of 'mangle_dupe_cols'\n",
      "  return pd.read_csv(xopen(filepath_or_buffer, \"rb\", use_auth_token=use_auth_token), **kwargs)\n",
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /home/aparna/.cache/huggingface/datasets/csv/default-18d6eb5fdf4808e6/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 431.82it/s]\n",
      "Map:   0%|          | 0/8791 [00:00<?, ? examples/s]/tmp/ipykernel_141077/3378448442.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_ids = torch.tensor(input_ids).squeeze()\n",
      "/tmp/ipykernel_141077/3378448442.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attention_mask = torch.tensor(attention_mask).squeeze()\n",
      "/tmp/ipykernel_141077/3378448442.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_ids = torch.tensor(target_ids).squeeze()\n",
      "/tmp/ipykernel_141077/3378448442.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_attention_mask = torch.tensor(target_attention_mask).squeeze()\n",
      "                                                                 \r"
     ]
    }
   ],
   "source": [
    "#tokenize train, val, test\n",
    "train = load_dataset('csv', data_files='train.csv')\n",
    "val = load_dataset('csv', data_files='val.csv')\n",
    "test = load_dataset('csv', data_files='test.csv')\n",
    "train = train.map(tokenize_df, batched=True, batch_size=128,remove_columns=['sentence','english_translation'])\n",
    "val = val.map(tokenize_df, batched=True, batch_size=128,remove_columns=['sentence','english_translation'])\n",
    "test = test.map(tokenize_df, batched=True, batch_size=128,remove_columns=['sentence','english_translation'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n",
      "512\n"
     ]
    }
   ],
   "source": [
    "train\n",
    "#get sample \n",
    "sample = train['train'][0]\n",
    "sample\n",
    "#print shapes\n",
    "print(len(sample['input_ids']))\n",
    "print(len(sample['attention_mask']))\n",
    "#print(len(sample['decoder_input_ids']))\n",
    "#print(len(sample['decoder_attention_mask']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 2198\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "# batch_size = 8\n",
    "# train_dataloader = DataLoader(\n",
    "\n",
    "#             train,  # The training samples.\n",
    "\n",
    "#             sampler = RandomSampler(train), # Select batches randomly\n",
    "\n",
    "#             batch_size = batch_size # Trains with this batch size.\n",
    "\n",
    "#         )\n",
    "\n",
    "# validation_dataloader = DataLoader(\n",
    "\n",
    "#             val, # The validation samples.\n",
    "\n",
    "#             sampler = SequentialSampler(val), # Pull out batches sequentially.\n",
    "\n",
    "#             batch_size = batch_size # Evaluate with this batch size.\n",
    "\n",
    "#         )\n",
    "\n",
    "# test_dataloader = DataLoader(\n",
    "\n",
    "\n",
    "#             test, # The validation samples. \n",
    "\n",
    "#             sampler = SequentialSampler(test), # Pull out batches sequentially.\n",
    "\n",
    "#             batch_size = batch_size # Evaluate with this batch size.\n",
    "\n",
    "#         )\n",
    "\n",
    "# #test train data loader\n",
    "# for batch in train_dataloader:\n",
    "\n",
    "#     print(batch)\n",
    "\n",
    "#     break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "\n",
    "def tokenize_sentence(arg):\n",
    "    encoded_arg =tokenizer(arg)\n",
    "    return tokenizer.convert_ids_to_tokens(encoded_arg.input_ids)\n",
    "\n",
    "def metrics_func(eval_arg):\n",
    "    preds, labels = eval_arg\n",
    "    # Replace -100\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    # Convert id tokens to text\n",
    "    text_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    text_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    # Insert a line break (\\n) in each sentence for ROUGE scoring\n",
    "    # (Note : Please change this code, when you perform on other languages except for Japanese)\n",
    "    text_preds = [(p if p.endswith((\"!\", \"ï¼\", \"?\", \"ï¼Ÿ\", \"ã€‚\")) else p + \"ã€‚\") for p in text_preds]\n",
    "    text_labels = [(l if l.endswith((\"!\", \"ï¼\", \"?\", \"ï¼Ÿ\", \"ã€‚\")) else l + \"ã€‚\") for l in text_labels]\n",
    "    sent_tokenizer_jp = RegexpTokenizer(u'[^!ï¼?ï¼Ÿã€‚]*[!ï¼?ï¼Ÿã€‚]')\n",
    "    text_preds = [\"\\n\".join(np.char.strip(sent_tokenizer_jp.tokenize(p))) for p in text_preds]\n",
    "    text_labels = [\"\\n\".join(np.char.strip(sent_tokenizer_jp.tokenize(l))) for l in text_labels]\n",
    "    # compute ROUGE score with custom tokenization\n",
    "    return rouge_metric.compute(\n",
    "        predictions=text_preds,\n",
    "        references=text_labels,\n",
    "        tokenizer=tokenize_sentence\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(119552, 1024)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# finetuen mt5\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "#model = BertGenerationEncoder.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "\n",
    "\n",
    "encoder = BertGenerationEncoder.from_pretrained(\"bert-base-multilingual-cased\", bos_token_id=101, eos_token_id=102)\n",
    "# add cross attention layers and use BERT's cls token as BOS token and sep token as EOS token\n",
    "decoder = BertGenerationDecoder.from_pretrained(\n",
    "    \"bert-large-uncased\", add_cross_attention=True, is_decoder=True, bos_token_id=101, eos_token_id=102\n",
    ")\n",
    "model = EncoderDecoderModel(encoder=encoder, decoder=decoder)\n",
    "\n",
    "model.encoder.resize_token_embeddings(len(tokenizer))\n",
    "model.decoder.resize_token_embeddings(len(tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "  0%|          | 0/2740 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The model did not return a loss from the inputs, only the following keys: last_hidden_state. For reference, the inputs it received are input_ids,attention_mask.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 38\u001b[0m\n\u001b[1;32m     27\u001b[0m trainer \u001b[39m=\u001b[39m Seq2SeqTrainer(\n\u001b[1;32m     28\u001b[0m     model\u001b[39m=\u001b[39mmodel,                         \u001b[39m# the instantiated ðŸ¤— Transformers model to be trained\u001b[39;00m\n\u001b[1;32m     29\u001b[0m     args\u001b[39m=\u001b[39mtraining_args,                  \u001b[39m# training arguments, defined above\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \n\u001b[1;32m     35\u001b[0m )\n\u001b[1;32m     37\u001b[0m \u001b[39m#train\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n\u001b[1;32m     40\u001b[0m \u001b[39m#save model\u001b[39;00m\n\u001b[1;32m     41\u001b[0m trainer\u001b[39m.\u001b[39msave_model(\u001b[39m\"\u001b[39m\u001b[39m./mbert\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/transformers/trainer.py:1543\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1538\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[1;32m   1540\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1541\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1542\u001b[0m )\n\u001b[0;32m-> 1543\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1544\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1545\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1546\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1547\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1548\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/transformers/trainer.py:1791\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1789\u001b[0m         tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   1790\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1791\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs)\n\u001b[1;32m   1793\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   1794\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1795\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1796\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1797\u001b[0m ):\n\u001b[1;32m   1798\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1799\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/transformers/trainer.py:2539\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2536\u001b[0m     \u001b[39mreturn\u001b[39;00m loss_mb\u001b[39m.\u001b[39mreduce_mean()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m   2538\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 2539\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_loss(model, inputs)\n\u001b[1;32m   2541\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mn_gpu \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   2542\u001b[0m     loss \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mmean()  \u001b[39m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/transformers/trainer.py:2584\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2582\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2583\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(outputs, \u001b[39mdict\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m outputs:\n\u001b[0;32m-> 2584\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   2585\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mThe model did not return a loss from the inputs, only the following keys: \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2586\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m,\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(outputs\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m. For reference, the inputs it received are \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m,\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(inputs\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2587\u001b[0m         )\n\u001b[1;32m   2588\u001b[0m     \u001b[39m# We don't use .loss here since the model may return tuples instead of ModelOutput.\u001b[39;00m\n\u001b[1;32m   2589\u001b[0m     loss \u001b[39m=\u001b[39m outputs[\u001b[39m\"\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(outputs, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m outputs[\u001b[39m0\u001b[39m]\n",
      "\u001b[0;31mValueError\u001b[0m: The model did not return a loss from the inputs, only the following keys: last_hidden_state. For reference, the inputs it received are input_ids,attention_mask."
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#training args\n",
    "\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "  output_dir = \"mbert-codemixed\",\n",
    "  log_level = \"error\",\n",
    "  num_train_epochs = 10,\n",
    "  learning_rate = 5e-4,\n",
    "  lr_scheduler_type = \"linear\",\n",
    "  warmup_steps = 90,\n",
    "  optim = \"adafactor\",\n",
    "  weight_decay = 0.01,\n",
    "  per_device_train_batch_size =2,\n",
    "  per_device_eval_batch_size = 1,\n",
    "  gradient_accumulation_steps = 16,\n",
    "  evaluation_strategy = \"steps\",\n",
    "  eval_steps = 100,\n",
    "  predict_with_generate=True,\n",
    "  generation_max_length = 128,\n",
    "  save_steps = 500,\n",
    "  logging_steps = 10,\n",
    "  push_to_hub = False\n",
    ")\n",
    "\n",
    "\n",
    "#trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train[\"train\"],         # training dataset\n",
    "    eval_dataset=val[\"train\"],             # evaluation dataset\n",
    "    tokenizer=tokenizer,               # tokenizer\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer, model=model), # data collator\n",
    "    \n",
    ")\n",
    "\n",
    "#train\n",
    "trainer.train()\n",
    "\n",
    "#save model\n",
    "trainer.save_model(\"./mbert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[     0,   1250,   7800, 198280,    259,  65334,    260,   1061,  39084,\n",
      "          19349,    318,  51571,  62342,  16263, 144044,    603,   2148,    513,\n",
      "           2941,    334,   1312,  88806,   2050,    432,    262,    268,    387,\n",
      "           1759,    290, 165794,      1,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0],\n",
      "        [     0,  27696,  62342,    259,    261,    342,    776, 113865,    714,\n",
      "           2829,    387, 102339,   9065,  42716,    260,   1816,    321,  12961,\n",
      "            623,   3663,    479,   1776,   1250,   6253, 182594,    262,    290,\n",
      "           1795,   1061, 146525,  56696,    313,  11395,    330,  35714,    609,\n",
      "           1350,    311,      1],\n",
      "        [     0,   1250,  38393,    265,    299,    609,    339,   4592,   6504,\n",
      "            259,   1542,    787,   3007,    288,   6313,    260,   1061,    559,\n",
      "            604,    263, 152418,   7925,   7954,      1,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0],\n",
      "        [     0,    387,  96125,   4748,  30875,   1711,    714,  16097,    339,\n",
      "            609,   6644,    288,    783,  58500,   5155,    281,   5169,  11495,\n",
      "              1,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0],\n",
      "        [     0,   1250,  16403, 210381,    584,  26112,  10060,   2050,    259,\n",
      "          97308,   1776,      1,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0]]) tensor([[  1250,   7800, 198280,  ...,      0,      0,      0],\n",
      "        [ 11674,   6101,   2050,  ...,      0,      0,      0],\n",
      "        [  1250,  38393,    265,  ...,      0,      0,      0],\n",
      "        [   387,  96125,   4748,  ...,      0,      0,      0],\n",
      "        [  1250,  16403, 210381,  ...,      0,      0,      0]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'rouge1': 0.47184469277492536,\n",
       " 'rouge2': 0.32362358952522885,\n",
       " 'rougeL': 0.44858887882143694,\n",
       " 'rougeLsum': 0.44858887882143694}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "model = MT5ForConditionalGeneration.from_pretrained(\"./mt5v1\")\n",
    "#tokenizer = MT5Tokenizer.from_pretrained(\"./mt5\")\n",
    "\n",
    "\n",
    "sample_dataloader = DataLoader(\n",
    "  test[\"train\"].with_format(\"torch\"),\n",
    "  collate_fn=DataCollatorForSeq2Seq(tokenizer, model=model),\n",
    "  batch_size=5)\n",
    "for batch in sample_dataloader:\n",
    "  with torch.no_grad():\n",
    "    preds = model.generate(\n",
    "      batch[\"input_ids\"],\n",
    "      num_beams=15,\n",
    "      num_return_sequences=1,\n",
    "      no_repeat_ngram_size=1,\n",
    "      remove_invalid_values=True,\n",
    "      max_length=128,\n",
    "    )\n",
    "  labels = batch[\"labels\"]\n",
    "  break\n",
    "print(preds, labels)\n",
    "metrics_func([preds, labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Input's Text *****\n",
      "@rynkee it is thi thought which we want to change. @PunsTurnMeOn\n",
      "***** codemix (True Value) *****\n",
      "@rynkee yehi soch to badalni hai @PunsTurnMeOn\n",
      "***** codemix (Generated Text) *****\n",
      "@rynkees it is thi thought which we want to change.@PunsTurnMeOn\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Predict with test data (first 5 rows)\n",
    "sample_dataloader = DataLoader(\n",
    "  test[\"train\"].with_format(\"torch\"),\n",
    "  collate_fn=DataCollatorForSeq2Seq(tokenizer, model=model),\n",
    "  batch_size=5)\n",
    "for batch in sample_dataloader:\n",
    "  with torch.no_grad():\n",
    "    preds = model.generate(\n",
    "      batch[\"input_ids\"],\n",
    "      num_beams=15,\n",
    "      num_return_sequences=1,\n",
    "      no_repeat_ngram_size=1,\n",
    "      remove_invalid_values=True,\n",
    "      max_length=128,\n",
    "    )\n",
    "  labels = batch[\"labels\"]\n",
    "  inputs = batch[\"input_ids\"]\n",
    "  break\n",
    "\n",
    "# Replace -100 (see above)\n",
    "inputs = np.where(inputs != -100, inputs, tokenizer.pad_token_id)\n",
    "labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "\n",
    "# Convert id tokens to text\n",
    "text_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "text_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "text_inputs = tokenizer.batch_decode(inputs, skip_special_tokens=True)\n",
    "\n",
    "# Show result\n",
    "print(\"***** Input's Text *****\")\n",
    "print(text_inputs[2])\n",
    "print(\"***** codemix (True Value) *****\")\n",
    "print(text_labels[2])\n",
    "print(\"***** codemix (Generated Text) *****\")\n",
    "print(text_preds[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Input's Text *****\n",
      "@hurdangi haan.. @sagarikaghose sister will eat green mango today @the_hindu\n",
      "***** codemix (True Value) *****\n",
      "@hurdangi haan.. @sagarikaghose Didi aaj hare rang ke aam khaengi @the_hindu\n",
      "***** codemix (Generated Text) *****\n",
      "@hurdangi haan.@sagarikaghose bhai green mango peene ke saath kharab kar jaao #the_hindu\n",
      "***** Input's Text *****\n",
      "wait brother, do not cry this much, its #GST not a bomb. have some shame. @digvijaya_28 @INCIndia \" country brought it out \"now you sit and cry\n",
      "***** codemix (True Value) *****\n",
      "Are bas kar bhai itna nahi rone \"ka #GST hai bomb nahi. Kuch to sharm karo. @digvijaya_28 @INCIndia \" desh nikal liya \"aage u sit and cry\n",
      "***** codemix (Generated Text) *****\n",
      "wait bhai, do not cry this much #GST nahi bomb. Haan kuch ho chuka hai @digvijaya_28@INCIndia \" country brought it out\"\n",
      "***** Input's Text *****\n",
      "@rynkee it is thi thought which we want to change. @PunsTurnMeOn\n",
      "***** codemix (True Value) *****\n",
      "@rynkee yehi soch to badalni hai @PunsTurnMeOn\n",
      "***** codemix (Generated Text) *****\n",
      "@rynkees it is thi thought which we want to change.@PunsTurnMeOn\n",
      "***** Input's Text *****\n",
      "#RightToPrivacy under this act, is it possible to have urinals in open space\n",
      "***** codemix (True Value) *****\n",
      "#RightToPrivacy ke andar ab khule me susu allow hai kya?\n",
      "***** codemix (Generated Text) *****\n",
      "#RightToPrivacy under this act is it possible to have urinals in open space\n",
      "***** Input's Text *****\n",
      "@FreeCharge recharge too.. fun too#MyCaptionForFreeCharge\n",
      "***** codemix (True Value) *****\n",
      "@FreeCharge recharge bhi.. Maze bhi #MyCaptionForFreeCharge\n",
      "***** codemix (Generated Text) *****\n",
      "@FreeCharge recharge bhi kar rahe hai\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(\"***** Input's Text *****\")\n",
    "    print(text_inputs[i])\n",
    "    print(\"***** codemix (True Value) *****\")\n",
    "    print(text_labels[i])\n",
    "    print(\"***** codemix (Generated Text) *****\")\n",
    "    print(text_preds[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
