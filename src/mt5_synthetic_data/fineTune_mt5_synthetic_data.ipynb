{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "# fine tune mt5 on dataset\n",
    "from transformers import MT5ForConditionalGeneration, MT5Tokenizer\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from simpletransformers.t5 import T5Model, T5Args\n",
    "from transformers import pipeline\n",
    "#import train split\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.preprocessing\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from google.transliteration import transliterate_word\n",
    "import klib\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>en</th>\n",
       "      <th>cm</th>\n",
       "      <th>prefix</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Other, Private Use</td>\n",
       "      <td>nizI</td>\n",
       "      <td>translate English to Hinglish:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[SCREAMING]</td>\n",
       "      <td>barTa chillA</td>\n",
       "      <td>translate English to Hinglish:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Spouse</td>\n",
       "      <td>pati/patnI</td>\n",
       "      <td>translate English to Hinglish:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I will never salute you!</td>\n",
       "      <td>maiM will kabhI Apako salAma</td>\n",
       "      <td>translate English to Hinglish:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>and the stars and the trees bow themselves;</td>\n",
       "      <td>aura the stars aura the bUTiyA.N beleM themselves</td>\n",
       "      <td>translate English to Hinglish:</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            en  \\\n",
       "0                           Other, Private Use   \n",
       "1                                  [SCREAMING]   \n",
       "2                                       Spouse   \n",
       "3                     I will never salute you!   \n",
       "4  and the stars and the trees bow themselves;   \n",
       "\n",
       "                                                  cm  \\\n",
       "0                                               nizI   \n",
       "1                                       barTa chillA   \n",
       "2                                         pati/patnI   \n",
       "3                       maiM will kabhI Apako salAma   \n",
       "4  aura the stars aura the bUTiyA.N beleM themselves   \n",
       "\n",
       "                            prefix  \n",
       "0  translate English to Hinglish:   \n",
       "1  translate English to Hinglish:   \n",
       "2  translate English to Hinglish:   \n",
       "3  translate English to Hinglish:   \n",
       "4  translate English to Hinglish:   "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load dataset\n",
    "#English-Hindi code-mixed parallel corpus.csv\n",
    "df = pd.read_json('sentences_0.json')\n",
    "df = df.dropna()\n",
    "df = df.reset_index(drop=True)\n",
    "# add column for prefix\n",
    "df['prefix'] = 'translate English to Hinglish: '\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of cleaned data: (34147, 2) - Remaining NAs: 0\n",
      "\n",
      "\n",
      "Dropped rows: 5500\n",
      "     of which 5500 duplicates. (Rows (first 150 shown): [180, 375, 556, 566, 651, 801, 845, 866, 874, 1113, 1209, 1219, 1382, 1397, 1514, 1532, 1597, 1727, 1791, 1833, 1839, 1874, 1936, 1940, 1947, 1968, 1994, 2009, 2022, 2032, 2091, 2132, 2139, 2142, 2151, 2161, 2195, 2217, 2223, 2242, 2297, 2318, 2343, 2346, 2358, 2410, 2420, 2450, 2476, 2521, 2543, 2545, 2563, 2607, 2608, 2616, 2653, 2689, 2716, 2811, 2831, 2839, 2867, 2877, 3007, 3019, 3034, 3075, 3088, 3095, 3115, 3122, 3182, 3227, 3253, 3260, 3315, 3327, 3329, 3428, 3468, 3569, 3570, 3583, 3597, 3610, 3653, 3674, 3690, 3693, 3695, 3736, 3756, 3760, 3784, 3867, 3880, 3884, 3888, 3897, 3961, 4048, 4068, 4077, 4122, 4180, 4208, 4228, 4304, 4311, 4342, 4368, 4374, 4400, 4403, 4411, 4460, 4466, 4496, 4520, 4523, 4535, 4552, 4556, 4559, 4567, 4580, 4593, 4597, 4635, 4646, 4648, 4663, 4677, 4691, 4696, 4717, 4727, 4766, 4787, 4825, 4847, 4865, 4896, 4904, 4924, 4928, 4932, 4961, 4966])\n",
      "\n",
      "Dropped columns: 1\n",
      "     of which 1 single valued.     Columns: ['prefix']\n",
      "Dropped missing values: 0\n",
      "Reduced memory by at least: 0.39 MB (-42.86%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#data cleaning \n",
    "\n",
    "df=klib.data_cleaning(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21853\n",
      "5464\n",
      "6830\n"
     ]
    }
   ],
   "source": [
    "#split train, val, test\n",
    "# convert df  so that it can be used by transformers\n",
    "\n",
    "\n",
    "train, test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "train, val = train_test_split(train, test_size=0.2, random_state=42)\n",
    "train = train.reset_index(drop=True)\n",
    "val = val.reset_index(drop=True)\n",
    "test = test.reset_index(drop=True)\n",
    "\n",
    "#print lens\n",
    "print(len(train))\n",
    "print(len(val))\n",
    "print(len(test))\n",
    "\n",
    "#save train, val, test\n",
    "train.to_csv('train1.csv', index=False)\n",
    "val.to_csv('val1.csv', index=False)\n",
    "test.to_csv('test1.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['en', 'cm'], dtype='object')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tokenize\n",
    "tokenizer = MT5Tokenizer.from_pretrained(\"google/mt5-small\")\n",
    "tokenizer.add_special_tokens({'additional_special_tokens': ['<sep>']})\n",
    "tokenizer.add_special_tokens({'additional_special_tokens': ['<pad>']})\n",
    "tokenizer.add_special_tokens({'additional_special_tokens': ['<s>']})\n",
    "tokenizer.add_special_tokens({'additional_special_tokens': ['</s>']})\n",
    "tokenizer.add_special_tokens({'additional_special_tokens': ['<unk>']})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['en']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 512\n",
    "def tokenize_df(df):\n",
    "    target = tokenizer([str(i) for i in df[\"cm\"]], padding='max_length', truncation=True, return_tensors=\"pt\", max_length=maxlen)\n",
    "    input = tokenizer([str(i) for i in df[\"en\"]], padding='max_length', truncation=True, return_tensors=\"pt\", max_length=maxlen)\n",
    "    #print(\"x\")\n",
    "    input_ids = input['input_ids']\n",
    "    attention_mask = input['attention_mask']\n",
    "    target_ids = target['input_ids']\n",
    "    target_attention_mask = target['attention_mask']\n",
    "    decoder_input_ids = target_ids.clone()\n",
    "    #convert to tensors\n",
    "    input_ids = torch.tensor(input_ids).squeeze()\n",
    "    attention_mask = torch.tensor(attention_mask).squeeze()\n",
    "    target_ids = torch.tensor(target_ids).squeeze()\n",
    "    target_attention_mask = torch.tensor(target_attention_mask).squeeze()\n",
    "   # decoder_input_ids = torch.tensor(decoder_input_ids)\n",
    "    \n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "        'labels': target_ids,\n",
    "        #'decoder_input_ids': decoder_input_ids,\n",
    "        #'decoder_attention_mask': target_attention_mask\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:717\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors\u001b[0;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[1;32m    716\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_tensor(value):\n\u001b[0;32m--> 717\u001b[0m     tensor \u001b[39m=\u001b[39m as_tensor(value)\n\u001b[1;32m    719\u001b[0m     \u001b[39m# Removing this for now in favor of controlling the shape with `prepend_batch_axis`\u001b[39;00m\n\u001b[1;32m    720\u001b[0m     \u001b[39m# # at-least2d\u001b[39;00m\n\u001b[1;32m    721\u001b[0m     \u001b[39m# if tensor.ndim > 2:\u001b[39;00m\n\u001b[1;32m    722\u001b[0m     \u001b[39m#     tensor = tensor.squeeze(0)\u001b[39;00m\n\u001b[1;32m    723\u001b[0m     \u001b[39m# elif tensor.ndim < 2:\u001b[39;00m\n\u001b[1;32m    724\u001b[0m     \u001b[39m#     tensor = tensor[None, :]\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tokenize_df(df)\n",
      "Cell \u001b[0;32mIn[41], line 3\u001b[0m, in \u001b[0;36mtokenize_df\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtokenize_df\u001b[39m(df):\n\u001b[0;32m----> 3\u001b[0m     target \u001b[39m=\u001b[39m tokenizer([\u001b[39mstr\u001b[39;49m(i) \u001b[39mfor\u001b[39;49;00m i \u001b[39min\u001b[39;49;00m df[\u001b[39m\"\u001b[39;49m\u001b[39mcm\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m.\u001b[39;49mvalues], padding\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mmax_length\u001b[39;49m\u001b[39m'\u001b[39;49m, truncation\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, return_tensors\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mpt\u001b[39;49m\u001b[39m\"\u001b[39;49m, max_length\u001b[39m=\u001b[39;49mmaxlen)\n\u001b[1;32m      4\u001b[0m     \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m tokenizer([\u001b[39mstr\u001b[39m(i) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m df[\u001b[39m\"\u001b[39m\u001b[39men\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mvalues], padding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmax_length\u001b[39m\u001b[39m'\u001b[39m, truncation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m, max_length\u001b[39m=\u001b[39mmaxlen)\n\u001b[1;32m      5\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mx\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2523\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2521\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_in_target_context_manager:\n\u001b[1;32m   2522\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_switch_to_input_mode()\n\u001b[0;32m-> 2523\u001b[0m     encodings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_one(text\u001b[39m=\u001b[39;49mtext, text_pair\u001b[39m=\u001b[39;49mtext_pair, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mall_kwargs)\n\u001b[1;32m   2524\u001b[0m \u001b[39mif\u001b[39;00m text_target \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2525\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2609\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2604\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   2605\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbatch length of `text`: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(text)\u001b[39m}\u001b[39;00m\u001b[39m does not match batch length of `text_pair`:\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2606\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(text_pair)\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2607\u001b[0m         )\n\u001b[1;32m   2608\u001b[0m     batch_text_or_text_pairs \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(text, text_pair)) \u001b[39mif\u001b[39;00m text_pair \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m text\n\u001b[0;32m-> 2609\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_encode_plus(\n\u001b[1;32m   2610\u001b[0m         batch_text_or_text_pairs\u001b[39m=\u001b[39;49mbatch_text_or_text_pairs,\n\u001b[1;32m   2611\u001b[0m         add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[1;32m   2612\u001b[0m         padding\u001b[39m=\u001b[39;49mpadding,\n\u001b[1;32m   2613\u001b[0m         truncation\u001b[39m=\u001b[39;49mtruncation,\n\u001b[1;32m   2614\u001b[0m         max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[1;32m   2615\u001b[0m         stride\u001b[39m=\u001b[39;49mstride,\n\u001b[1;32m   2616\u001b[0m         is_split_into_words\u001b[39m=\u001b[39;49mis_split_into_words,\n\u001b[1;32m   2617\u001b[0m         pad_to_multiple_of\u001b[39m=\u001b[39;49mpad_to_multiple_of,\n\u001b[1;32m   2618\u001b[0m         return_tensors\u001b[39m=\u001b[39;49mreturn_tensors,\n\u001b[1;32m   2619\u001b[0m         return_token_type_ids\u001b[39m=\u001b[39;49mreturn_token_type_ids,\n\u001b[1;32m   2620\u001b[0m         return_attention_mask\u001b[39m=\u001b[39;49mreturn_attention_mask,\n\u001b[1;32m   2621\u001b[0m         return_overflowing_tokens\u001b[39m=\u001b[39;49mreturn_overflowing_tokens,\n\u001b[1;32m   2622\u001b[0m         return_special_tokens_mask\u001b[39m=\u001b[39;49mreturn_special_tokens_mask,\n\u001b[1;32m   2623\u001b[0m         return_offsets_mapping\u001b[39m=\u001b[39;49mreturn_offsets_mapping,\n\u001b[1;32m   2624\u001b[0m         return_length\u001b[39m=\u001b[39;49mreturn_length,\n\u001b[1;32m   2625\u001b[0m         verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m   2626\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   2627\u001b[0m     )\n\u001b[1;32m   2628\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2629\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencode_plus(\n\u001b[1;32m   2630\u001b[0m         text\u001b[39m=\u001b[39mtext,\n\u001b[1;32m   2631\u001b[0m         text_pair\u001b[39m=\u001b[39mtext_pair,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2647\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   2648\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2800\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2790\u001b[0m \u001b[39m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m   2791\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   2792\u001b[0m     padding\u001b[39m=\u001b[39mpadding,\n\u001b[1;32m   2793\u001b[0m     truncation\u001b[39m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2797\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   2798\u001b[0m )\n\u001b[0;32m-> 2800\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_batch_encode_plus(\n\u001b[1;32m   2801\u001b[0m     batch_text_or_text_pairs\u001b[39m=\u001b[39;49mbatch_text_or_text_pairs,\n\u001b[1;32m   2802\u001b[0m     add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[1;32m   2803\u001b[0m     padding_strategy\u001b[39m=\u001b[39;49mpadding_strategy,\n\u001b[1;32m   2804\u001b[0m     truncation_strategy\u001b[39m=\u001b[39;49mtruncation_strategy,\n\u001b[1;32m   2805\u001b[0m     max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[1;32m   2806\u001b[0m     stride\u001b[39m=\u001b[39;49mstride,\n\u001b[1;32m   2807\u001b[0m     is_split_into_words\u001b[39m=\u001b[39;49mis_split_into_words,\n\u001b[1;32m   2808\u001b[0m     pad_to_multiple_of\u001b[39m=\u001b[39;49mpad_to_multiple_of,\n\u001b[1;32m   2809\u001b[0m     return_tensors\u001b[39m=\u001b[39;49mreturn_tensors,\n\u001b[1;32m   2810\u001b[0m     return_token_type_ids\u001b[39m=\u001b[39;49mreturn_token_type_ids,\n\u001b[1;32m   2811\u001b[0m     return_attention_mask\u001b[39m=\u001b[39;49mreturn_attention_mask,\n\u001b[1;32m   2812\u001b[0m     return_overflowing_tokens\u001b[39m=\u001b[39;49mreturn_overflowing_tokens,\n\u001b[1;32m   2813\u001b[0m     return_special_tokens_mask\u001b[39m=\u001b[39;49mreturn_special_tokens_mask,\n\u001b[1;32m   2814\u001b[0m     return_offsets_mapping\u001b[39m=\u001b[39;49mreturn_offsets_mapping,\n\u001b[1;32m   2815\u001b[0m     return_length\u001b[39m=\u001b[39;49mreturn_length,\n\u001b[1;32m   2816\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m   2817\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   2818\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/transformers/tokenization_utils.py:737\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    734\u001b[0m     second_ids \u001b[39m=\u001b[39m get_input_ids(pair_ids) \u001b[39mif\u001b[39;00m pair_ids \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    735\u001b[0m     input_ids\u001b[39m.\u001b[39mappend((first_ids, second_ids))\n\u001b[0;32m--> 737\u001b[0m batch_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_batch_prepare_for_model(\n\u001b[1;32m    738\u001b[0m     input_ids,\n\u001b[1;32m    739\u001b[0m     add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[1;32m    740\u001b[0m     padding_strategy\u001b[39m=\u001b[39;49mpadding_strategy,\n\u001b[1;32m    741\u001b[0m     truncation_strategy\u001b[39m=\u001b[39;49mtruncation_strategy,\n\u001b[1;32m    742\u001b[0m     max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[1;32m    743\u001b[0m     stride\u001b[39m=\u001b[39;49mstride,\n\u001b[1;32m    744\u001b[0m     pad_to_multiple_of\u001b[39m=\u001b[39;49mpad_to_multiple_of,\n\u001b[1;32m    745\u001b[0m     return_attention_mask\u001b[39m=\u001b[39;49mreturn_attention_mask,\n\u001b[1;32m    746\u001b[0m     return_token_type_ids\u001b[39m=\u001b[39;49mreturn_token_type_ids,\n\u001b[1;32m    747\u001b[0m     return_overflowing_tokens\u001b[39m=\u001b[39;49mreturn_overflowing_tokens,\n\u001b[1;32m    748\u001b[0m     return_special_tokens_mask\u001b[39m=\u001b[39;49mreturn_special_tokens_mask,\n\u001b[1;32m    749\u001b[0m     return_length\u001b[39m=\u001b[39;49mreturn_length,\n\u001b[1;32m    750\u001b[0m     return_tensors\u001b[39m=\u001b[39;49mreturn_tensors,\n\u001b[1;32m    751\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m    752\u001b[0m )\n\u001b[1;32m    754\u001b[0m \u001b[39mreturn\u001b[39;00m BatchEncoding(batch_outputs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/transformers/tokenization_utils.py:817\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._batch_prepare_for_model\u001b[0;34m(self, batch_ids_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_length, verbose)\u001b[0m\n\u001b[1;32m    807\u001b[0m         batch_outputs[key]\u001b[39m.\u001b[39mappend(value)\n\u001b[1;32m    809\u001b[0m batch_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpad(\n\u001b[1;32m    810\u001b[0m     batch_outputs,\n\u001b[1;32m    811\u001b[0m     padding\u001b[39m=\u001b[39mpadding_strategy\u001b[39m.\u001b[39mvalue,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    814\u001b[0m     return_attention_mask\u001b[39m=\u001b[39mreturn_attention_mask,\n\u001b[1;32m    815\u001b[0m )\n\u001b[0;32m--> 817\u001b[0m batch_outputs \u001b[39m=\u001b[39m BatchEncoding(batch_outputs, tensor_type\u001b[39m=\u001b[39;49mreturn_tensors)\n\u001b[1;32m    819\u001b[0m \u001b[39mreturn\u001b[39;00m batch_outputs\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:210\u001b[0m, in \u001b[0;36mBatchEncoding.__init__\u001b[0;34m(self, data, encoding, tensor_type, prepend_batch_axis, n_sequences)\u001b[0m\n\u001b[1;32m    206\u001b[0m     n_sequences \u001b[39m=\u001b[39m encoding[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mn_sequences\n\u001b[1;32m    208\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_sequences \u001b[39m=\u001b[39m n_sequences\n\u001b[0;32m--> 210\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_to_tensors(tensor_type\u001b[39m=\u001b[39;49mtensor_type, prepend_batch_axis\u001b[39m=\u001b[39;49mprepend_batch_axis)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:733\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors\u001b[0;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[1;32m    728\u001b[0m         \u001b[39mif\u001b[39;00m key \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39moverflowing_tokens\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    729\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    730\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mUnable to create tensor returning overflowing tokens of different lengths. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    731\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mPlease see if a fast version of this tokenizer is available to have this feature available.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    732\u001b[0m             )\n\u001b[0;32m--> 733\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    734\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mUnable to create tensor, you should probably activate truncation and/or padding with\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    735\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39mpadding=True\u001b[39m\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39mtruncation=True\u001b[39m\u001b[39m'\u001b[39m\u001b[39m to have batched tensors with the same length. Perhaps your\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    736\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m features (`\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m` in this case) have excessive nesting (inputs type `list` where type `int` is\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    737\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m expected).\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    738\u001b[0m         )\n\u001b[1;32m    740\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected)."
     ]
    }
   ],
   "source": [
    "tokenize_df(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (/home/aparna/.cache/huggingface/datasets/csv/default-d1b7b873547cdb08/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n",
      "100%|██████████| 1/1 [00:00<00:00, 658.55it/s]\n",
      "Found cached dataset csv (/home/aparna/.cache/huggingface/datasets/csv/default-2ec1d98a11ac1402/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n",
      "100%|██████████| 1/1 [00:00<00:00, 659.27it/s]\n",
      "Found cached dataset csv (/home/aparna/.cache/huggingface/datasets/csv/default-1d357afce8d6b0aa/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n",
      "100%|██████████| 1/1 [00:00<00:00, 647.07it/s]\n",
      "Map:   0%|          | 0/21853 [00:00<?, ? examples/s]/tmp/ipykernel_142914/1243960566.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_ids = torch.tensor(input_ids).squeeze()\n",
      "/tmp/ipykernel_142914/1243960566.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attention_mask = torch.tensor(attention_mask).squeeze()\n",
      "/tmp/ipykernel_142914/1243960566.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_ids = torch.tensor(target_ids).squeeze()\n",
      "/tmp/ipykernel_142914/1243960566.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_attention_mask = torch.tensor(target_attention_mask).squeeze()\n",
      "Map:   2%|▏         | 512/21853 [00:00<00:09, 2211.15 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      "x\n",
      "x\n",
      "x\n",
      "x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   5%|▍         | 1024/21853 [00:00<00:08, 2428.68 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      "x\n",
      "x\n",
      "x\n",
      "x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   9%|▉         | 1920/21853 [00:00<00:07, 2588.13 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      "x\n",
      "x\n",
      "x\n",
      "x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  11%|█         | 2432/21853 [00:00<00:07, 2562.37 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      "x\n",
      "x\n",
      "x\n",
      "x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  14%|█▍        | 3072/21853 [00:01<00:07, 2558.27 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      "x\n",
      "x\n",
      "x\n",
      "x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  17%|█▋        | 3712/21853 [00:01<00:07, 2532.34 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      "x\n",
      "x\n",
      "x\n",
      "x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  19%|█▉        | 4224/21853 [00:01<00:06, 2637.83 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      "x\n",
      "x\n",
      "x\n",
      "x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  23%|██▎       | 5120/21853 [00:01<00:06, 2681.23 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      "x\n",
      "x\n",
      "x\n",
      "x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  25%|██▌       | 5504/21853 [00:02<00:06, 2601.03 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      "x\n",
      "x\n",
      "x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  29%|██▊       | 6272/21853 [00:02<00:06, 2562.63 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      "x\n",
      "x\n",
      "x\n",
      "x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  30%|███       | 6656/21853 [00:02<00:06, 2491.83 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      "x\n",
      "x\n",
      "x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  32%|███▏      | 7040/21853 [00:02<00:06, 2466.58 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      "x\n",
      "x\n",
      "x\n",
      "x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  36%|███▋      | 7936/21853 [00:03<00:05, 2427.98 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      "x\n",
      "x\n",
      "x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  37%|███▋      | 8192/21853 [00:03<00:05, 2399.93 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      "x\n",
      "x\n",
      "x\n",
      "x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  41%|████      | 8960/21853 [00:03<00:05, 2550.25 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      "x\n",
      "x\n",
      "x\n",
      "x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  45%|████▍     | 9728/21853 [00:03<00:04, 2654.45 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      "x\n",
      "x\n",
      "x\n",
      "x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  46%|████▋     | 10112/21853 [00:03<00:04, 2683.43 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      "x\n",
      "x\n",
      "x\n",
      "x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  50%|█████     | 11008/21853 [00:04<00:03, 2780.12 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      "x\n",
      "x\n",
      "x\n",
      "x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  53%|█████▎    | 11520/21853 [00:04<00:03, 2676.07 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      "x\n",
      "x\n",
      "x\n",
      "x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  54%|█████▍    | 11904/21853 [00:04<00:03, 2695.99 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      "x\n",
      "x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  58%|█████▊    | 12672/21853 [00:05<00:04, 2180.39 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      "x\n",
      "x\n",
      "x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  60%|██████    | 13184/21853 [00:05<00:03, 2306.68 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      "x\n",
      "x\n",
      "x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  63%|██████▎   | 13696/21853 [00:05<00:03, 2303.82 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      "x\n",
      "x\n",
      "x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  64%|██████▍   | 13952/21853 [00:05<00:04, 1934.01 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      "x\n",
      "x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  67%|██████▋   | 14592/21853 [00:05<00:03, 2190.83 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      "x\n",
      "x\n",
      "x\n",
      "x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  69%|██████▉   | 15104/21853 [00:06<00:02, 2271.81 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      "x\n",
      "x\n",
      "x\n",
      "x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  73%|███████▎  | 16000/21853 [00:06<00:02, 2534.26 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      "x\n",
      "x\n",
      "x\n",
      "x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  74%|███████▍  | 16256/21853 [00:06<00:02, 2445.10 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      "x\n",
      "x\n",
      "x\n",
      "x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  78%|███████▊  | 17152/21853 [00:06<00:01, 2629.73 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      "x\n",
      "x\n",
      "x\n",
      "x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  80%|████████  | 17536/21853 [00:07<00:01, 2555.94 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      "x\n",
      "x\n",
      "x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  83%|████████▎ | 18176/21853 [00:07<00:01, 2444.25 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      "x\n",
      "x\n",
      "x\n",
      "x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  86%|████████▌ | 18816/21853 [00:07<00:01, 2500.89 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      "x\n",
      "x\n",
      "x\n",
      "x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  88%|████████▊ | 19328/21853 [00:07<00:00, 2647.13 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      "x\n",
      "x\n",
      "x\n",
      "x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  93%|█████████▎| 20352/21853 [00:08<00:00, 2631.76 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      "x\n",
      "x\n",
      "x\n",
      "x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  95%|█████████▍| 20736/21853 [00:08<00:00, 2563.43 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      "x\n",
      "x\n",
      "x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  98%|█████████▊| 21376/21853 [00:08<00:00, 2554.14 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      "x\n",
      "x\n",
      "x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  99%|█████████▉| 21632/21853 [00:08<00:00, 2443.80 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      "x\n",
      "x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/5464 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      "x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   7%|▋         | 384/5464 [00:00<00:02, 2333.08 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      "x\n",
      "x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  14%|█▍        | 768/5464 [00:00<00:01, 2567.92 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      "x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  21%|██        | 1152/5464 [00:00<00:01, 2573.94 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      "x\n",
      "x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  28%|██▊       | 1536/5464 [00:00<00:01, 2587.93 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      "x\n",
      "x\n",
      "x\n",
      "x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  37%|███▋      | 2048/5464 [00:00<00:01, 2641.57 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      "x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  45%|████▍     | 2432/5464 [00:00<00:01, 2698.19 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      "x\n",
      "x\n",
      "x\n",
      "x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  54%|█████▍    | 2944/5464 [00:01<00:00, 2635.91 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      "x\n",
      "x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  63%|██████▎   | 3456/5464 [00:01<00:00, 2695.53 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      "x\n",
      "x\n",
      "x\n",
      "x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  73%|███████▎  | 3968/5464 [00:01<00:00, 2717.94 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      "x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  80%|███████▉  | 4352/5464 [00:01<00:00, 2624.90 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      "x\n",
      "x\n",
      "x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  89%|████████▉ | 4864/5464 [00:01<00:00, 2599.08 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      "x\n",
      "x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  96%|█████████▌| 5248/5464 [00:01<00:00, 2632.74 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      "x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      "x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   4%|▎         | 256/6830 [00:00<00:02, 2254.45 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      "x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   7%|▋         | 512/6830 [00:00<00:02, 2293.93 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      "x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  11%|█         | 768/6830 [00:00<00:02, 2142.17 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      "x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  15%|█▍        | 1024/6830 [00:00<00:02, 2148.82 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      "x\n",
      "x\n",
      "x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  22%|██▏       | 1536/6830 [00:00<00:02, 2334.41 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      "x\n",
      "x\n",
      "x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  26%|██▌       | 1792/6830 [00:00<00:02, 2121.27 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      "x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  32%|███▏      | 2176/6830 [00:00<00:02, 2283.44 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      "x\n",
      "x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  39%|███▉      | 2688/6830 [00:01<00:01, 2421.93 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      "x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  43%|████▎     | 2944/6830 [00:01<00:01, 2325.47 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      "x\n",
      "x\n",
      "x\n",
      "x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  49%|████▊     | 3328/6830 [00:01<00:01, 2457.11 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      "x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  54%|█████▍    | 3712/6830 [00:01<00:01, 2521.18 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      "x\n",
      "x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  62%|██████▏   | 4224/6830 [00:01<00:00, 2693.03 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      "x\n",
      "x\n",
      "x\n",
      "x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  69%|██████▉   | 4736/6830 [00:01<00:00, 2704.95 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      "x\n",
      "x\n",
      "x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  75%|███████▍  | 5120/6830 [00:02<00:00, 2385.65 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  84%|████████▍ | 5760/6830 [00:02<00:00, 2468.58 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      "x\n",
      "x\n",
      "x\n",
      "x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  88%|████████▊ | 6016/6830 [00:02<00:00, 2397.11 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      "x\n",
      "x\n",
      "x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  94%|█████████▎| 6400/6830 [00:02<00:00, 2454.34 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  97%|█████████▋| 6656/6830 [00:02<00:00, 2478.29 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      "x\n",
      "x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    }
   ],
   "source": [
    "#tokenize train, val, test\n",
    "train = load_dataset('csv', data_files='train1.csv')\n",
    "val = load_dataset('csv', data_files='val1.csv')\n",
    "test = load_dataset('csv', data_files='test1.csv')\n",
    "train = train.map(tokenize_df, batched=True, batch_size=128,remove_columns=['en','cm'])\n",
    "val = val.map(tokenize_df, batched=True, batch_size=128,remove_columns=['en','cm'])\n",
    "test = test.map(tokenize_df, batched=True, batch_size=128,remove_columns=['en','cm'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'en': 'I get into the shower, I told you already.',\n",
       " 'cm': 'maiM get into the maiM snAna told you already .'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = train['train'][0]\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n",
      "512\n"
     ]
    }
   ],
   "source": [
    "train\n",
    "#get sample \n",
    "sample = train['train'][0]\n",
    "sample\n",
    "#print shapes\n",
    "print(len(sample['input_ids']))\n",
    "print(len(sample['attention_mask']))\n",
    "#print(len(sample['decoder_input_ids']))\n",
    "#print(len(sample['decoder_attention_mask']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 2198\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "# batch_size = 8\n",
    "# train_dataloader = DataLoader(\n",
    "\n",
    "#             train,  # The training samples.\n",
    "\n",
    "#             sampler = RandomSampler(train), # Select batches randomly\n",
    "\n",
    "#             batch_size = batch_size # Trains with this batch size.\n",
    "\n",
    "#         )\n",
    "\n",
    "# validation_dataloader = DataLoader(\n",
    "\n",
    "#             val, # The validation samples.\n",
    "\n",
    "#             sampler = SequentialSampler(val), # Pull out batches sequentially.\n",
    "\n",
    "#             batch_size = batch_size # Evaluate with this batch size.\n",
    "\n",
    "#         )\n",
    "\n",
    "# test_dataloader = DataLoader(\n",
    "\n",
    "\n",
    "#             test, # The validation samples. \n",
    "\n",
    "#             sampler = SequentialSampler(test), # Pull out batches sequentially.\n",
    "\n",
    "#             batch_size = batch_size # Evaluate with this batch size.\n",
    "\n",
    "#         )\n",
    "\n",
    "# #test train data loader\n",
    "# for batch in train_dataloader:\n",
    "\n",
    "#     print(batch)\n",
    "\n",
    "#     break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /home/aparna/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--rouge/b01e0accf3bd6dd24839b769a5fda24e14995071570870922c71970b3a6ed886 (last modified on Mon Mar 20 18:02:43 2023) since it couldn't be found locally at evaluate-metric--rouge, or remotely on the Hugging Face Hub.\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "\n",
    "def tokenize_sentence(arg):\n",
    "    encoded_arg =tokenizer(arg)\n",
    "    return tokenizer.convert_ids_to_tokens(encoded_arg.input_ids)\n",
    "\n",
    "def metrics_func(eval_arg):\n",
    "    preds, labels = eval_arg\n",
    "    # Replace -100\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    # Convert id tokens to text\n",
    "    text_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    text_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    # Insert a line break (\\n) in each sentence for ROUGE scoring\n",
    "    # (Note : Please change this code, when you perform on other languages except for Japanese)\n",
    "    text_preds = [(p if p.endswith((\"!\", \"！\", \"?\", \"？\", \"。\")) else p + \"。\") for p in text_preds]\n",
    "    text_labels = [(l if l.endswith((\"!\", \"！\", \"?\", \"？\", \"。\")) else l + \"。\") for l in text_labels]\n",
    "    sent_tokenizer_jp = RegexpTokenizer(u'[^!！?？。]*[!！?？。]')\n",
    "    text_preds = [\"\\n\".join(np.char.strip(sent_tokenizer_jp.tokenize(p))) for p in text_preds]\n",
    "    text_labels = [\"\\n\".join(np.char.strip(sent_tokenizer_jp.tokenize(l))) for l in text_labels]\n",
    "    # compute ROUGE score with custom tokenization\n",
    "    return rouge_metric.compute(\n",
    "        predictions=text_preds,\n",
    "        references=text_labels,\n",
    "        tokenizer=tokenize_sentence\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/2740 [12:55<196:31:16, 258.49s/it]\n"
     ]
    }
   ],
   "source": [
    "# finetuen mt5\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "model = MT5ForConditionalGeneration.from_pretrained('bert-base-multilingual-cased')\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "#training args\n",
    "\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "  output_dir = \"mt5-synthetic\",\n",
    "  log_level = \"error\",\n",
    "  num_train_epochs = 10,\n",
    "  learning_rate = 5e-4,\n",
    "  lr_scheduler_type = \"linear\",\n",
    "  warmup_steps = 90,\n",
    "  optim = \"adafactor\",\n",
    "  weight_decay = 0.01,\n",
    "  per_device_train_batch_size = 2,\n",
    "  per_device_eval_batch_size = 1,\n",
    "  gradient_accumulation_steps = 16,\n",
    "  evaluation_strategy = \"steps\",\n",
    "  eval_steps = 100,\n",
    "  predict_with_generate=True,\n",
    "  generation_max_length = 128,\n",
    "  save_steps = 500,\n",
    "  logging_steps = 10,\n",
    "  push_to_hub = False\n",
    ")\n",
    "\n",
    "\n",
    "#trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train[\"train\"],         # training dataset\n",
    "    eval_dataset=val[\"train\"],             # evaluation dataset\n",
    "    tokenizer=tokenizer,               # tokenizer\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer, model=model), # data collator\n",
    "    \n",
    ")\n",
    "\n",
    "#train\n",
    "trainer.train()\n",
    "\n",
    "#save model\n",
    "trainer.save_model(\"./mt5_synthetic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[     0,   1250,   7800, 198280,    259,  65334,    260,   1061,  39084,\n",
      "          19349,    318,  51571,  62342,  16263, 144044,    603,   2148,    513,\n",
      "           2941,    334,   1312,  88806,   2050,    432,    262,    268,    387,\n",
      "           1759,    290, 165794,      1,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0],\n",
      "        [     0,  27696,  62342,    259,    261,    342,    776, 113865,    714,\n",
      "           2829,    387, 102339,   9065,  42716,    260,   1816,    321,  12961,\n",
      "            623,   3663,    479,   1776,   1250,   6253, 182594,    262,    290,\n",
      "           1795,   1061, 146525,  56696,    313,  11395,    330,  35714,    609,\n",
      "           1350,    311,      1],\n",
      "        [     0,   1250,  38393,    265,    299,    609,    339,   4592,   6504,\n",
      "            259,   1542,    787,   3007,    288,   6313,    260,   1061,    559,\n",
      "            604,    263, 152418,   7925,   7954,      1,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0],\n",
      "        [     0,    387,  96125,   4748,  30875,   1711,    714,  16097,    339,\n",
      "            609,   6644,    288,    783,  58500,   5155,    281,   5169,  11495,\n",
      "              1,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0],\n",
      "        [     0,   1250,  16403, 210381,    584,  26112,  10060,   2050,    259,\n",
      "          97308,   1776,      1,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0]]) tensor([[  1250,   7800, 198280,  ...,      0,      0,      0],\n",
      "        [ 11674,   6101,   2050,  ...,      0,      0,      0],\n",
      "        [  1250,  38393,    265,  ...,      0,      0,      0],\n",
      "        [   387,  96125,   4748,  ...,      0,      0,      0],\n",
      "        [  1250,  16403, 210381,  ...,      0,      0,      0]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'rouge1': 0.47184469277492536,\n",
       " 'rouge2': 0.32362358952522885,\n",
       " 'rougeL': 0.44858887882143694,\n",
       " 'rougeLsum': 0.44858887882143694}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "model = MT5ForConditionalGeneration.from_pretrained(\"./mt5_synthetic\")\n",
    "#tokenizer = MT5Tokenizer.from_pretrained(\"./mt5\")\n",
    "\n",
    "\n",
    "sample_dataloader = DataLoader(\n",
    "  test[\"train\"].with_format(\"torch\"),\n",
    "  collate_fn=DataCollatorForSeq2Seq(tokenizer, model=model),\n",
    "  batch_size=5)\n",
    "for batch in sample_dataloader:\n",
    "  with torch.no_grad():\n",
    "    preds = model.generate(\n",
    "      batch[\"input_ids\"],\n",
    "      num_beams=15,\n",
    "      num_return_sequences=1,\n",
    "      no_repeat_ngram_size=1,\n",
    "      remove_invalid_values=True,\n",
    "      max_length=128,\n",
    "    )\n",
    "  labels = batch[\"labels\"]\n",
    "  break\n",
    "print(preds, labels)\n",
    "metrics_func([preds, labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Input's Text *****\n",
      "@rynkee it is thi thought which we want to change. @PunsTurnMeOn\n",
      "***** codemix (True Value) *****\n",
      "@rynkee yehi soch to badalni hai @PunsTurnMeOn\n",
      "***** codemix (Generated Text) *****\n",
      "@rynkees it is thi thought which we want to change.@PunsTurnMeOn\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Predict with test data (first 5 rows)\n",
    "sample_dataloader = DataLoader(\n",
    "  test[\"train\"].with_format(\"torch\"),\n",
    "  collate_fn=DataCollatorForSeq2Seq(tokenizer, model=model),\n",
    "  batch_size=5)\n",
    "for batch in sample_dataloader:\n",
    "  with torch.no_grad():\n",
    "    preds = model.generate(\n",
    "      batch[\"input_ids\"],\n",
    "      num_beams=15,\n",
    "      num_return_sequences=1,\n",
    "      no_repeat_ngram_size=1,\n",
    "      remove_invalid_values=True,\n",
    "      max_length=128,\n",
    "    )\n",
    "  labels = batch[\"labels\"]\n",
    "  inputs = batch[\"input_ids\"]\n",
    "  break\n",
    "\n",
    "# Replace -100 (see above)\n",
    "inputs = np.where(inputs != -100, inputs, tokenizer.pad_token_id)\n",
    "labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "\n",
    "# Convert id tokens to text\n",
    "text_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "text_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "text_inputs = tokenizer.batch_decode(inputs, skip_special_tokens=True)\n",
    "\n",
    "# Show result\n",
    "print(\"***** Input's Text *****\")\n",
    "print(text_inputs[2])\n",
    "print(\"***** codemix (True Value) *****\")\n",
    "print(text_labels[2])\n",
    "print(\"***** codemix (Generated Text) *****\")\n",
    "print(text_preds[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Input's Text *****\n",
      "@hurdangi haan.. @sagarikaghose sister will eat green mango today @the_hindu\n",
      "***** codemix (True Value) *****\n",
      "@hurdangi haan.. @sagarikaghose Didi aaj hare rang ke aam khaengi @the_hindu\n",
      "***** codemix (Generated Text) *****\n",
      "@hurdangi haan.@sagarikaghose bhai green mango peene ke saath kharab kar jaao #the_hindu\n",
      "***** Input's Text *****\n",
      "wait brother, do not cry this much, its #GST not a bomb. have some shame. @digvijaya_28 @INCIndia \" country brought it out \"now you sit and cry\n",
      "***** codemix (True Value) *****\n",
      "Are bas kar bhai itna nahi rone \"ka #GST hai bomb nahi. Kuch to sharm karo. @digvijaya_28 @INCIndia \" desh nikal liya \"aage u sit and cry\n",
      "***** codemix (Generated Text) *****\n",
      "wait bhai, do not cry this much #GST nahi bomb. Haan kuch ho chuka hai @digvijaya_28@INCIndia \" country brought it out\"\n",
      "***** Input's Text *****\n",
      "@rynkee it is thi thought which we want to change. @PunsTurnMeOn\n",
      "***** codemix (True Value) *****\n",
      "@rynkee yehi soch to badalni hai @PunsTurnMeOn\n",
      "***** codemix (Generated Text) *****\n",
      "@rynkees it is thi thought which we want to change.@PunsTurnMeOn\n",
      "***** Input's Text *****\n",
      "#RightToPrivacy under this act, is it possible to have urinals in open space\n",
      "***** codemix (True Value) *****\n",
      "#RightToPrivacy ke andar ab khule me susu allow hai kya?\n",
      "***** codemix (Generated Text) *****\n",
      "#RightToPrivacy under this act is it possible to have urinals in open space\n",
      "***** Input's Text *****\n",
      "@FreeCharge recharge too.. fun too#MyCaptionForFreeCharge\n",
      "***** codemix (True Value) *****\n",
      "@FreeCharge recharge bhi.. Maze bhi #MyCaptionForFreeCharge\n",
      "***** codemix (Generated Text) *****\n",
      "@FreeCharge recharge bhi kar rahe hai\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(\"***** Input's Text *****\")\n",
    "    print(text_inputs[i])\n",
    "    print(\"***** codemix (True Value) *****\")\n",
    "    print(text_labels[i])\n",
    "    print(\"***** codemix (Generated Text) *****\")\n",
    "    print(text_preds[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
